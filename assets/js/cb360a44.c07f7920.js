"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[4753],{85162:(n,e,t)=>{t.d(e,{Z:()=>o});var r=t(67294),l=t(86010);const a="tabItem_Ymn6";function o(n){let{children:e,hidden:t,className:o}=n;return r.createElement("div",{role:"tabpanel",className:(0,l.Z)(a,o),hidden:t},e)}},65488:(n,e,t)=>{t.d(e,{Z:()=>m});var r=t(87462),l=t(67294),a=t(86010),o=t(72389),s=t(67392),i=t(7094),_=t(12466);const d="tabList__CuJ",c="tabItem_LNqP";function p(n){const{lazy:e,block:t,defaultValue:o,values:p,groupId:m,className:u}=n,h=l.Children.map(n.children,(n=>{if((0,l.isValidElement)(n)&&"value"in n.props)return n;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof n.type?n.type:n.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),f=p??h.map((n=>{let{props:{value:e,label:t,attributes:r}}=n;return{value:e,label:t,attributes:r}})),k=(0,s.l)(f,((n,e)=>n.value===e.value));if(k.length>0)throw new Error(`Docusaurus error: Duplicate values "${k.map((n=>n.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const y=null===o?o:o??h.find((n=>n.props.default))?.props.value??h[0].props.value;if(null!==y&&!f.some((n=>n.value===y)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${y}" but none of its children has the corresponding value. Available values are: ${f.map((n=>n.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:x,setTabGroupChoices:v}=(0,i.U)(),[w,b]=(0,l.useState)(y),g=[],{blockElementScrollPositionUntilNextRender:N}=(0,_.o5)();if(null!=m){const n=x[m];null!=n&&n!==w&&f.some((e=>e.value===n))&&b(n)}const B=n=>{const e=n.currentTarget,t=g.indexOf(e),r=f[t].value;r!==w&&(N(e),b(r),null!=m&&v(m,String(r)))},T=n=>{let e=null;switch(n.key){case"Enter":B(n);break;case"ArrowRight":{const t=g.indexOf(n.currentTarget)+1;e=g[t]??g[0];break}case"ArrowLeft":{const t=g.indexOf(n.currentTarget)-1;e=g[t]??g[g.length-1];break}}e?.focus()};return l.createElement("div",{className:(0,a.Z)("tabs-container",d)},l.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.Z)("tabs",{"tabs--block":t},u)},f.map((n=>{let{value:e,label:t,attributes:o}=n;return l.createElement("li",(0,r.Z)({role:"tab",tabIndex:w===e?0:-1,"aria-selected":w===e,key:e,ref:n=>g.push(n),onKeyDown:T,onClick:B},o,{className:(0,a.Z)("tabs__item",c,o?.className,{"tabs__item--active":w===e})}),t??e)}))),e?(0,l.cloneElement)(h.filter((n=>n.props.value===w))[0],{className:"margin-top--md"}):l.createElement("div",{className:"margin-top--md"},h.map(((n,e)=>(0,l.cloneElement)(n,{key:e,hidden:n.props.value!==w})))))}function m(n){const e=(0,o.Z)();return l.createElement(p,(0,r.Z)({key:String(e)},n))}},9566:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>_,default:()=>u,frontMatter:()=>i,metadata:()=>d,toc:()=>p});var r=t(87462),l=(t(67294),t(3905)),a=t(44212),o=t(65488),s=t(85162);const i={id:"pytorch-resnet",title:"",sidebar_label:"ResNet"},_=void 0,d={unversionedId:"computer/cv/ml-dl/pytorch/pytorch-resnet",id:"computer/cv/ml-dl/pytorch/pytorch-resnet",title:"",description:"",source:"@site/docs/computer/cv/ml-dl/pytorch/pytorch-resnet.md",sourceDirName:"computer/cv/ml-dl/pytorch",slug:"/computer/cv/ml-dl/pytorch/pytorch-resnet",permalink:"/personal-site/docs/computer/cv/ml-dl/pytorch/pytorch-resnet",draft:!1,editUrl:"https://github.com/rcxxx/docs/tree/master/docs/computer/cv/ml-dl/pytorch/pytorch-resnet.md",tags:[],version:"current",frontMatter:{id:"pytorch-resnet",title:"",sidebar_label:"ResNet"},sidebar:"\ud83d\udc40CV",previous:{title:"Install",permalink:"/personal-site/docs/computer/cv/ml-dl/pytorch/pytorch-install"},next:{title:"YOLO",permalink:"/personal-site/docs/category/YOLO"}},c={},p=[{value:"ResNet \u7f51\u7edc\u7ed3\u6784",id:"resnet-\u7f51\u7edc\u7ed3\u6784",level:3},{value:"ResNet \u7684\u6b8b\u5dee\u7ed3\u6784",id:"resnet-\u7684\u6b8b\u5dee\u7ed3\u6784",level:4},{value:"\u57fa\u672c\u6b8b\u5dee\u5757",id:"\u57fa\u672c\u6b8b\u5dee\u5757",level:4},{value:"ResNet \u7f51\u7edc",id:"resnet-\u7f51\u7edc",level:3},{value:"PyTorch \u5b9e\u73b0 ResNet",id:"pytorch-\u5b9e\u73b0-resnet",level:3},{value:"\u57fa\u7840\u6a21\u5757",id:"\u57fa\u7840\u6a21\u5757",level:4},{value:"\u7f51\u7edc\u5806\u53e0",id:"\u7f51\u7edc\u5806\u53e0",level:4},{value:"\u6784\u5efa\u7f51\u7edc",id:"\u6784\u5efa\u7f51\u7edc",level:4},{value:"\u5b8c\u6574\u4ee3\u7801\u5730\u5740",id:"\u5b8c\u6574\u4ee3\u7801\u5730\u5740",level:4},{value:"\u53c2\u8003",id:"\u53c2\u8003",level:2}],m={toc:p};function u(n){let{components:e,...t}=n;return(0,l.kt)("wrapper",(0,r.Z)({},m,t,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("h3",{id:"resnet-\u7f51\u7edc\u7ed3\u6784"},"ResNet \u7f51\u7edc\u7ed3\u6784"),(0,l.kt)("h4",{id:"resnet-\u7684\u6b8b\u5dee\u7ed3\u6784"},"ResNet \u7684\u6b8b\u5dee\u7ed3\u6784"),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/yolov5_resnet/resnet-building-block.png",alt:null})),(0,l.kt)("h4",{id:"\u57fa\u672c\u6b8b\u5dee\u5757"},"\u57fa\u672c\u6b8b\u5dee\u5757"),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/yolov5_resnet/resnet-block.png",alt:null})),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"BasicBlock")," \u7ecf\u8fc7\u4e24\u4e2a 3x3 \u7684\u5377\u79ef\u5c42\uff0c\u518d\u4e0e\u8f93\u5165\u76f8\u52a0\u540e\u6fc0\u6d3b\u8f93\u51fa")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"BottleNeck")," \u7528\u4e8e\u66f4\u6df1\u5c42\u7684 ",(0,l.kt)("inlineCode",{parentName:"p"},"resnet")," \u7f51\u7edc\uff0c\u5148\u7ecf\u8fc7\u4e00\u4e2a 1x1 \u3001\u4e00\u4e2a 3x3 \u7684\u5377\u79ef\u5c42\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u4e2a 1x1 \u7684\u5377\u79ef\u5c42\u6539\u53d8\u901a\u9053\u6570\uff0c\u518d\u4e0e\u8f93\u5165\u76f8\u52a0\u540e\u6fc0\u6d3b\u8f93\u51fa"))),(0,l.kt)("hr",null),(0,l.kt)("h3",{id:"resnet-\u7f51\u7edc"},"ResNet \u7f51\u7edc"),(0,l.kt)("p",null,"ResNet \u7684\u7f51\u7edc\u7ed3\u6784\u5c31\u662f\u901a\u8fc7\u57fa\u7840\u6b8b\u5dee\u5757\u53e0\u52a0\u6765\u642d\u5efa"),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/yolov5_resnet/resnet-net-build.png",alt:null})),(0,l.kt)("hr",null),(0,l.kt)("h3",{id:"pytorch-\u5b9e\u73b0-resnet"},"PyTorch \u5b9e\u73b0 ResNet"),(0,l.kt)("h4",{id:"\u57fa\u7840\u6a21\u5757"},"\u57fa\u7840\u6a21\u5757"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},(0,l.kt)("inlineCode",{parentName:"strong"},"1x1")," \u5377\u79ef\u5757")),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["Conv2D_1x1"]\ncode["def conv1x1()"]\n\nstyle A fill:#ffc20e,stroke:#333,stroke-width:2px\nstyle code fill:#90d7ec,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"def conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, \n                     out_planes, \n                     stride=stride,\n                     kernel_size=1, \n                     bias=False)\n"))),(0,l.kt)(s.Z,{value:"C++",label:"C++",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-cpp"},"static torch::nn::Conv2dOptions create_conv1x1_options(int64_t in_planes,\n                                                       int64_t out_planes,\n                                                       int64_t stride = 1)\n{\n    torch::nn::Conv2dOptions conv_opt = create_conv_options(\n            in_planes\n            , out_planes\n            , 1\n            , stride\n            , 0 /*padding */\n            , 1  /*groups */\n            , 1 /*dilation */\n            , false);\n    return conv_opt;\n}\n")))),(0,l.kt)("hr",null),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},(0,l.kt)("inlineCode",{parentName:"strong"},"3x3")," \u5377\u79ef\u5757")),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["Conv2D_3x3"]\ncode["def conv3x3()"]\n\nstyle A fill:#f58220,stroke:#333,stroke-width:2px\nstyle code fill:#90d7ec,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"def conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=1,\n                     bias=False)\n"))),(0,l.kt)(s.Z,{value:"C++",label:"C++",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-cpp"},"static torch::nn::Conv2dOptions create_conv3x3_options(int64_t in_planes,\n                                                       int64_t out_planes,\n                                                       int64_t stride = 1,\n                                                       int64_t groups = 1,\n                                                       int64_t dilation = 1)\n{\n    torch::nn::Conv2dOptions conv_opt = create_conv_options(\n            in_planes,\n            out_planes,\n            3, /* kernel_size */\n            stride,\n            dilation, /* padding */\n            groups,\n            dilation, /* dilation */\n            false);\n    return conv_opt;\n}\n")))),(0,l.kt)("hr",null),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},(0,l.kt)("inlineCode",{parentName:"strong"},"BasicBlock")," \u6b8b\u5dee\u5757")),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["BasicBlock"]\nstyle A fill:#78cdd1,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(a.G,{chart:'flowchart LR;\ncode["class BasicBlock(nn.Module)"] --\x3e init["def __init__()"] \ncode --\x3e forward["def forward()"]\n\nstyle code fill:#90d7ec,stroke:#333,stroke-width:2px\nstyle init fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle forward fill:#ea66a6,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, down_sample=None):\n        super(BasicBlock, self).__init__()\n        self.conv_1 = conv3x3(in_planes, planes, stride)\n        self.bn_1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n        self.conv_2 = conv3x3(planes, planes)\n        self.bn_2 = nn.BatchNorm2d(planes)\n        self.down_sample = down_sample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv_1(x)\n        out = self.bn_1(out)\n        out = self.relu(out)\n        out = self.conv_2(out)\n        out = self.bn_2(out)\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n"))),(0,l.kt)(s.Z,{value:"C++",label:"C++",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-cpp"},'struct BasicBlock : torch::nn::Module {\n    BasicBlock(int64_t in_planes, int64_t planes, int64_t stride = 1,\n               torch::nn::Sequential down_sample = torch::nn::Sequential(),\n               int64_t groups = 1, int64_t base_width = 64,\n               int64_t dilation = 1)\n    {\n        if ((groups != 1) || (base_width != 64))\n        {\n            throw std::invalid_argument{\n                    "BasicBlock only supports groups=1 and base_width=64"};\n        }\n        if (dilation > 1)\n        {\n            throw std::invalid_argument{\n                    "Dilation > 1 not supported in BasicBlock"};\n        }\n        m_conv_1 = register_module("conv_1", torch::nn::Conv2d{create_conv3x3_options(in_planes, planes, stride)});\n        m_bn_1   = register_module("bn_1", torch::nn::BatchNorm2d{planes});\n        m_relu   = register_module("relu", torch::nn::ReLU{true});\n        m_conv_2 = register_module("conv_2", torch::nn::Conv2d{create_conv3x3_options(planes, planes)});\n        m_bn_2   = register_module("bn_2", torch::nn::BatchNorm2d{planes});\n        if (!down_sample->is_empty())\n        {\n            m_down_sample = register_module("down_sample", down_sample);\n        }\n        m_stride = stride;\n    }\n\n    static const int64_t m_expansion = 1;\n\n    torch::nn::Conv2d       m_conv_1{nullptr};\n    torch::nn::Conv2d       m_conv_2{nullptr};\n    torch::nn::BatchNorm2d  m_bn_1{nullptr};\n    torch::nn::BatchNorm2d  m_bn_2{nullptr};\n    torch::nn::ReLU         m_relu{nullptr};\n    torch::nn::Sequential   m_down_sample = torch::nn::Sequential();\n\n    int64_t m_stride;\n\n    torch::Tensor forward(const torch::Tensor& x)\n    {\n        torch::Tensor identity = x;\n\n        torch::Tensor out;\n        out = m_conv_1  -> forward(x);\n        out = m_bn_1    -> forward(out);\n        out = m_relu    -> forward(out);\n\n        out = m_conv_2  -> forward(out);\n        out = m_bn_2    -> forward(out);\n\n        if (!m_down_sample->is_empty())\n        {\n            identity = m_down_sample -> forward(x);\n        }\n\n        out += identity;\n        out = m_relu    -> forward(out);\n\n        return out;\n    }\n};\n')))),(0,l.kt)("hr",null),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},(0,l.kt)("inlineCode",{parentName:"strong"},"BottleNeck")," \u6b8b\u5dee\u5757")),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["BottleNeck"]\nstyle A fill:#78cdd1,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(a.G,{chart:'flowchart LR;\ncode["class BottleNeck(nn.Module)"] --\x3e init["def __init__()"] \ncode --\x3e forward["def forward()"]\n\nstyle code fill:#90d7ec,stroke:#333,stroke-width:2px\nstyle init fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle forward fill:#ea66a6,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"class BottleNeck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, down_sample=None):\n        super(BottleNeck, self).__init__()\n        self.conv_1 = conv1x1(in_planes, planes)\n        self.bn_1 = nn.BatchNorm2d(planes)\n        self.conv_2 = conv3x3(planes,\n                                planes,\n                                stride=stride)\n        self.bn_2 = nn.BatchNorm2d(planes)\n        self.conv_3 = conv1x1(planes, planes * 4)\n        self.bn_3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv_1(x)\n        out = self.bn_1(out)\n        out = self.relu(out)\n\n        out = self.conv_2(out)\n        out = self.bn_2(out)\n        out = self.relu(out)\n\n        out = self.conv_3(out)\n        out = self.bn_3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n"))),(0,l.kt)(s.Z,{value:"C++",label:"C++",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-cpp"},'struct Bottleneck : torch::nn::Module\n{\n    Bottleneck(int64_t in_planes, int64_t planes, int64_t stride = 1,\n               torch::nn::Sequential down_sample = torch::nn::Sequential(),\n               int64_t groups = 1, int64_t base_width = 64,\n               int64_t dilation = 1)\n    {\n        int64_t width = planes * (base_width / 64) * groups;\n\n        m_conv_1 = register_module("conv_1",torch::nn::Conv2d{create_conv1x1_options(in_planes, width)});\n        m_bn_1   = register_module("bn_1",torch::nn::BatchNorm2d{width});\n        m_conv_2 = register_module("conv_2",torch::nn::Conv2d{create_conv3x3_options(width, width, stride, groups, dilation)});\n        m_bn_2   = register_module("bn_2",torch::nn::BatchNorm2d{width});\n        m_conv_3 = register_module("conv_3",torch::nn::Conv2d{create_conv1x1_options(width, planes * m_expansion)});\n        m_bn_3   = register_module("bn_3",torch::nn::BatchNorm2d{planes * m_expansion});\n        m_relu   = register_module("relu",torch::nn::ReLU{true});\n        if (!down_sample->is_empty())\n        {\n            m_down_sample = register_module("down_sample", down_sample);\n        }\n        m_stride = stride;\n    }\n\n    static const int64_t m_expansion = 4;\n\n    torch::nn::Conv2d       m_conv_1{nullptr};\n    torch::nn::Conv2d       m_conv_2{nullptr};\n    torch::nn::Conv2d       m_conv_3{nullptr};\n    torch::nn::BatchNorm2d  m_bn_1{nullptr};\n    torch::nn::BatchNorm2d  m_bn_2{nullptr};\n    torch::nn::BatchNorm2d  m_bn_3{nullptr};\n    torch::nn::ReLU         m_relu{nullptr};\n    torch::nn::Sequential   m_down_sample = torch::nn::Sequential();\n\n    int64_t m_stride;\n\n    torch::Tensor forward(const torch::Tensor& x)\n    {\n        torch::Tensor identity = x;\n\n        torch::Tensor out;\n        out = m_conv_1  -> forward(x);\n        out = m_bn_1    -> forward(out);\n        out = m_relu    -> forward(out);\n\n        out = m_conv_2  -> forward(out);\n        out = m_bn_2    -> forward(out);\n        out = m_relu    -> forward(out);\n\n        out = m_conv_3  -> forward(out);\n        out = m_bn_3    -> forward(out);\n\n        if (!m_down_sample->is_empty())\n        {\n            identity = m_down_sample -> forward(x);\n        }\n\n        out += identity;\n        out = m_relu    -> forward(out);\n\n        return out;\n    }\n};\n')))),(0,l.kt)("hr",null),(0,l.kt)("h4",{id:"\u7f51\u7edc\u5806\u53e0"},"\u7f51\u7edc\u5806\u53e0"),(0,l.kt)(a.G,{chart:'flowchart LR;\ncode["class ResNet(nn.Module)"] --\x3e init["def __init__()"] \ncode --\x3e _make_layer["def _make_layer()"]\ncode --\x3e forward["def forward()"]\n\nstyle code fill:#90d7ec,stroke:#333,stroke-width:2px\nstyle init fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle _make_layer fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle forward fill:#ea66a6,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)("p",null,"\u7f51\u7edc\u5c42\u6570\u901a\u8fc7 ",(0,l.kt)("inlineCode",{parentName:"p"},"layers[]")," \u8fdb\u884c\u8bbe\u7f6e"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"def __init__(self, block, layers, num_classes = 1000):\n    self.in_planes = 64\n    \xb7\xb7\xb7\n    self.layer_1 = self._make_layer(block, 64, layers[0])\n    self.layer_2 = self._make_layer(block, 128, layers[1], stride=2)\n    self.layer_3 = self._make_layer(block, 256, layers[2], stride=2)\n    self.layer_4 = self._make_layer(block, 512, layers[3], stride=2)\n    \xb7\xb7\xb7\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"def _make_layer(self, block, planes, blocks, stride=1):\n    \xb7\xb7\xb7\n    layers = []\n    layers.append(block(self.in_planes, planes, stride, down_sample))\n    self.in_planes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.in_planes, planes))\n\n    return nn.Sequential(*layers)\n")),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"class ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes = 1000):\n        self.in_planes = 64\n        super(ResNet, self).__init__()\n        self.conv_1 = nn.Conv2d(3,\n                                64,\n                                kernel_size=7,\n                                stride=2,\n                                padding=3,\n                                bias=False)\n        self.bn_1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer_1 = self._make_layer(block, 64, layers[0])\n        self.layer_2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer_3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer_4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                nn.init.uniform_(m.weight, 0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        down_sample = None\n        if stride != 1 or self.in_planes != planes * block.expansion:\n            down_sample = nn.Sequential(\n                nn.Conv2d(self.in_planes,\n                          planes * block.expansion,\n                          kernel_size=1,\n                          stride=stride,\n                          bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_planes, planes, stride, down_sample))\n        self.in_planes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.in_planes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv_1(x)\n        x = self.bn_1(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n        x = self.layer_3(x)\n        x = self.layer_4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n"))),(0,l.kt)(s.Z,{value:"C++",label:"C++",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-cpp"},'template <typename Block>\nstruct ResNet_Base : torch::nn::Module\n{\n    explicit ResNet_Base(const std::vector<int64_t>& layers\n        , int64_t num_classes = 1000\n        , bool zero_init_residual = false\n        , int64_t groups = 1\n        , int64_t width_per_group = 64\n        , std::vector<int64_t> replace_stride_with_dilation = {})\n    {\n        if (replace_stride_with_dilation.empty())\n        {\n            replace_stride_with_dilation = {false, false, false};\n        }\n        if (replace_stride_with_dilation.size() != 3)\n        {\n            throw std::invalid_argument{\n                "replace_stride_with_dilation should be empty or have exactly "\n                "three elements."\n            };\n        }\n        m_groups = groups;\n        m_base_width = width_per_group;\n\n        m_conv_1 = register_module("conv_1"\n                , torch::nn::Conv2d{create_conv_options(3           /*in_planes = */\n                                                        , m_in_planes   /*out_planes = */\n                                                        , 7             /*kernel_size = */\n                                                        , 2                 /*stride = */\n                                                        , 3               /*padding = */\n                                                        , 1                /*groups = */\n                                                        , 1                /*dilation = */\n                                                        , false)});           /*bias = */\n\n        m_bn_1 = register_module("bn_1", torch::nn::BatchNorm2d{m_in_planes});\n        m_relu = register_module("relu", torch::nn::ReLU{true});\n        m_max_pool = register_module("max_pool"\n                , torch::nn::MaxPool2d{torch::nn::MaxPool2dOptions({3, 3})\n                    .stride({2, 2})\n                    .padding({1, 1})});\n\n        m_layer_1 = register_module("layer_1"\n                , _makeLayer(64, layers.at(0)));\n        m_layer_2 = register_module("layer_2"\n                , _makeLayer(128, layers.at(1), 2, replace_stride_with_dilation.at(0)));\n        m_layer_3 = register_module("layer_3"\n                , _makeLayer(256, layers.at(2), 2, replace_stride_with_dilation.at(1)));\n        m_layer_4 = register_module("layer_4"\n                , _makeLayer(512, layers.at(3), 2, replace_stride_with_dilation.at(2)));\n\n        m_avg_pool = register_module("avg_pool"\n                , torch::nn::AdaptiveAvgPool2d(torch::nn::AdaptiveAvgPool2dOptions({1, 1})));\n\n        m_fc = register_module(\n                "fc", torch::nn::Linear(512 * Block::m_expansion, num_classes));\n\n        for (const auto& m : modules(false))\n        {\n            if (m->name() == "torch::nn::Conv2dImpl")\n            {\n                torch::OrderedDict<std::string, torch::Tensor> named_parameters = m->named_parameters(false);\n                torch::Tensor* ptr_w = named_parameters.find("weight");\n                torch::nn::init::kaiming_normal_(*ptr_w, 0, torch::kFanOut,\n                                                 torch::kReLU);\n            }\n            else if ((m->name() == "torch::nn::BatchNormImpl") || (m->name() == "torch::nn::GroupNormImpl"))\n            {\n                torch::OrderedDict<std::string, torch::Tensor> named_parameters;\n                named_parameters = m->named_parameters(false);\n                torch::Tensor* ptr_w = named_parameters.find("weight");\n                torch::nn::init::constant_(*ptr_w, 1.0);\n                torch::Tensor* ptr_b = named_parameters.find("bias");\n                torch::nn::init::constant_(*ptr_b, 0.0);\n            }\n        }\n\n        if (zero_init_residual)\n        {\n            for (const auto& m : modules(false))\n            {\n                if (m->name() == "Bottleneck")\n                {\n                    torch::OrderedDict<std::string, torch::Tensor> named_parameters;\n                    named_parameters = m->named_modules()["bn3"]->named_parameters(false);\n                    torch::Tensor* ptr_w = named_parameters.find("weight");\n                    torch::nn::init::constant_(*ptr_w, 0.0);\n                }\n                else if (m->name() == "BasicBlock")\n                {\n                    torch::OrderedDict<std::string, torch::Tensor> named_parameters;\n                    named_parameters = m->named_modules()["bn2"]->named_parameters(false);\n                    torch::Tensor* ptr_w = named_parameters.find("weight");\n                    torch::nn::init::constant_(*ptr_w, 0.0);\n                }\n            }\n        }\n    }\n\n    int64_t m_in_planes = 64;\n    int64_t m_dilation = 1;\n    int64_t m_groups = 1;\n    int64_t m_base_width = 64;\n\n    torch::nn::Conv2d               m_conv_1{nullptr};\n    torch::nn::BatchNorm2d          m_bn_1{nullptr};\n    torch::nn::ReLU                 m_relu{nullptr};\n    torch::nn::MaxPool2d            m_max_pool{nullptr};\n    torch::nn::Sequential           m_layer_1{nullptr};\n    torch::nn::Sequential           m_layer_2{nullptr};\n    torch::nn::Sequential           m_layer_3{nullptr};\n    torch::nn::Sequential           m_layer_4{nullptr};\n    torch::nn::AdaptiveAvgPool2d    m_avg_pool{nullptr};\n    torch::nn::Linear               m_fc{nullptr};\n\n    torch::nn::Sequential _makeLayer(int64_t planes, int64_t blocks,\n                                      int64_t stride = 1, bool dilate = false)\n    {\n        torch::nn::Sequential down_sample = torch::nn::Sequential();\n        int64_t previous_dilation = m_dilation;\n        if (dilate)\n        {\n            m_dilation *= stride;\n            stride = 1;\n        }\n        if ((stride != 1) || (m_in_planes != planes * Block::m_expansion))\n        {\n            down_sample = torch::nn::Sequential(\n                    torch::nn::Conv2d(create_conv1x1_options(m_in_planes\n                            , planes * Block::m_expansion\n                            , stride))\n                    , torch::nn::BatchNorm2d(planes * Block::m_expansion));\n        }\n\n        torch::nn::Sequential layers;\n\n        layers->push_back(Block(m_in_planes\n                                , planes\n                                , stride\n                                , down_sample\n                                , m_groups\n                                , m_base_width\n                                , previous_dilation));\n\n        m_in_planes = planes * Block::m_expansion;\n        for (int64_t i = 0; i < blocks; i++)\n        {\n            layers->push_back(Block(m_in_planes\n                                    , planes\n                                    , 1\n                                    , torch::nn::Sequential()\n                                    , m_groups\n                                    , m_base_width, m_dilation));\n        }\n\n        return layers;\n    }\n\n    torch::Tensor _forward_impl(torch::Tensor x)\n    {\n\n        x = m_conv_1->forward(x);\n        x = m_bn_1->forward(x);\n        x = m_relu->forward(x);\n        x = m_max_pool->forward(x);\n\n        x = m_layer_1->forward(x);\n        x = m_layer_2->forward(x);\n        x = m_layer_3->forward(x);\n        x = m_layer_4->forward(x);\n\n        x = m_avg_pool->forward(x);\n        x = torch::flatten(x, 1);\n        x = m_fc->forward(x);\n\n        return x;\n    }\n\n    torch::Tensor _forward_rm_fc(torch::Tensor x)\n    {\n\n        x = m_conv_1->forward(x);\n        x = m_bn_1->forward(x);\n        x = m_relu->forward(x);\n        x = m_max_pool->forward(x);\n\n        x = m_layer_1->forward(x);\n        x = m_layer_2->forward(x);\n        x = m_layer_3->forward(x);\n        x = m_layer_4->forward(x);\n\n        x = m_avg_pool->forward(x);\n        x = torch::flatten(x, 1);\n        x = torch::nn::functional::normalize(x, torch::nn::functional::NormalizeFuncOptions().dim(1));\n\n        return x;\n    }\n\n    torch::Tensor forward(torch::Tensor x) { return _forward_impl(x); }\n};\n')))),(0,l.kt)("hr",null),(0,l.kt)("h4",{id:"\u6784\u5efa\u7f51\u7edc"},"\u6784\u5efa\u7f51\u7edc"),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["def  resnet18()"]\nlayers["layers:[2, 2, 2, 2]"]\nstyle A fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle layers fill:#a3cf62,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["def  resnet34()"]\nlayers["layers:[3, 4, 6, 3]"]\nstyle A fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle layers fill:#a3cf62,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["def  resnet50()"]\nlayers["layers:[3, 4, 6, 3]"]\nstyle A fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle layers fill:#a3cf62,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(a.G,{chart:'flowchart TD;\nA["def resnet101()"]\nlayers["layers:[3, 4, 23, 3]"]\nstyle A fill:#ea66a6,stroke:#333,stroke-width:2px\nstyle layers fill:#a3cf62,stroke:#333,stroke-width:2px',mdxType:"Mermaid"}),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},"def resnet18():\n    model = ResNet(BasicBlock, [2, 2, 2, 2])\n    return model\n\n\ndef resnet34():\n    model = ResNet(BasicBlock, [3, 4, 6, 3])\n    return model\n\n\ndef resnet50():\n    model = ResNet(BottleNeck, [3, 4, 6, 3])\n    return model\n\n\ndef resnet101():\n    model = ResNet(BottleNeck, [3, 4, 23, 3])\n    return model\n"))),(0,l.kt)(s.Z,{value:"C++",label:"C++",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-cpp"},"template <class Block>\nstd::shared_ptr<ResNet_Base<Block>>\nresNet_base(const std::vector<int64_t>& layers\n        , int64_t num_classes = 1000\n        , bool zero_init_residual = false\n        , int64_t groups = 1\n        , int64_t width_per_group = 64\n        , const std::vector<int64_t>& replace_stride_with_dilation = {})\n{\n    std::shared_ptr<ResNet_Base<Block>> model = std::make_shared<ResNet_Base<Block>>(\n            layers\n            , num_classes\n            , zero_init_residual\n            , groups\n            , width_per_group\n            , replace_stride_with_dilation);\n\n    return model;\n}\n\nstatic std::shared_ptr<ResNet_Base<BasicBlock>>\nresNet18(int64_t num_classes = 1000\n        , bool zero_init_residual = false\n        , int64_t groups = 1\n        , int64_t width_per_group = 64\n        , const std::vector<int64_t>& replace_stride_with_dilation = {})\n{\n    const std::vector<int64_t> layers{2, 2, 2, 2};\n    std::shared_ptr<ResNet_Base<BasicBlock>> model = resNet_base<BasicBlock>(\n            layers\n            , num_classes\n            , zero_init_residual\n            , groups\n            , width_per_group\n            , replace_stride_with_dilation);\n\n    return model;\n}\n\nstatic std::shared_ptr<ResNet_Base<BasicBlock>>\nresNet34(int64_t num_classes = 1000\n        , bool zero_init_residual = false\n        , int64_t groups = 1\n        , int64_t width_per_group = 64\n        , const std::vector<int64_t>& replace_stride_with_dilation = {})\n{\n    const std::vector<int64_t> layers{3, 4, 6, 3};\n    std::shared_ptr<ResNet_Base<BasicBlock>> model = resNet_base<BasicBlock>(\n            layers\n            , num_classes\n            , zero_init_residual\n            , groups\n            , width_per_group\n            , replace_stride_with_dilation);\n\n    return model;\n}\n\nstatic std::shared_ptr<ResNet_Base<Bottleneck>>\nresNet50(int64_t num_classes = 1000\n        , bool zero_init_residual = false\n        , int64_t groups = 1\n        , int64_t width_per_group = 64\n        , const std::vector<int64_t>& replace_stride_with_dilation = {})\n{\n    const std::vector<int64_t> layers{3, 4, 6, 3};\n    std::shared_ptr<ResNet_Base<Bottleneck>> model = resNet_base<Bottleneck>(\n            layers\n            , num_classes\n            , zero_init_residual\n            , groups\n            , width_per_group\n            , replace_stride_with_dilation);\n\n    return model;\n}\n\nstatic std::shared_ptr<ResNet_Base<Bottleneck>>\nresnet101(int64_t num_classes = 1000\n        , bool zero_init_residual = false\n        , int64_t groups = 1\n        , int64_t width_per_group = 64\n        , const std::vector<int64_t>& replace_stride_with_dilation = {})\n{\n    const std::vector<int64_t> layers{3, 4, 23, 3};\n    std::shared_ptr<ResNet_Base<Bottleneck>> model = resNet_base<Bottleneck>(\n            layers\n            , num_classes\n            , zero_init_residual\n            , groups\n            , width_per_group\n            , replace_stride_with_dilation);\n\n    return model;\n}\n")))),(0,l.kt)("h4",{id:"\u5b8c\u6574\u4ee3\u7801\u5730\u5740"},"\u5b8c\u6574\u4ee3\u7801\u5730\u5740"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("a",{parentName:"strong",href:"https://github.com/rcxxx/torch-resnet"},"rcxxx/torch-resnet/py/model.py")))),(0,l.kt)("h2",{id:"\u53c2\u8003"},"\u53c2\u8003"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("a",{parentName:"strong",href:"https://arxiv.org/abs/1512.03385"},"Deep Residual Learning for Image Recognition"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("a",{parentName:"strong",href:"https://pytorch.org/hub/pytorch_vision_resnet/"},"Deep residual networks pre-trained on ImageNet"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("a",{parentName:"strong",href:"https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py"},"vision/torchvision/models/resnet.py"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("a",{parentName:"strong",href:"https://zhuanlan.zhihu.com/p/31852747"},"\u4f60\u5fc5\u987b\u8981\u77e5\u9053CNN\u6a21\u578b\uff1aResNet")))))}u.isMDXComponent=!0}}]);