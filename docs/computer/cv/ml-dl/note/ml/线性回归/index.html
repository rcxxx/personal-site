<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-computer/cv/ml-dl/note/ml/线性回归">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.21">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-rh="true">机器学习的简单示例 —— 线性回归 | Rcxxx&#x27;s Personal Site</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="机器学习的简单示例 —— 线性回归 | Rcxxx&#x27;s Personal Site"><meta data-rh="true" name="description" content="线性回归（Linear Regression） 是机器学习和统计学中最基础广泛饮用的模型，是一种对自变量和因变量之间关系进行建模的回归分析"><meta data-rh="true" property="og:description" content="线性回归（Linear Regression） 是机器学习和统计学中最基础广泛饮用的模型，是一种对自变量和因变量之间关系进行建模的回归分析"><link data-rh="true" rel="icon" href="/personal-site/./img/icons/lzumi-icon-05-128x128.jpg"><link data-rh="true" rel="canonical" href="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归"><link data-rh="true" rel="alternate" href="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归" hreflang="en"><link data-rh="true" rel="alternate" href="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归" hreflang="x-default"><link rel="stylesheet" href="/personal-site/assets/css/styles.a67e7cdb.css">
<link rel="preload" href="/personal-site/assets/js/runtime~main.f246873e.js" as="script">
<link rel="preload" href="/personal-site/assets/js/main.6ada5662.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/personal-site/"><div class="navbar__logo"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a aria-current="page" class="navbar__item navbar__link heafer-user-icon navbar__link--active" href="/personal-site/docs/"></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">📝Docs</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/personal-site/docs/category/devices">💻 PC</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/C-C_plus_plus">⌨️ programming</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/OpenCV">👀CV</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/RC-RM">🎖️robot</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/Fusion 360">🔨3D Modeling</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">📚书单</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/personal-site/cs-book-list">⌨️ 计算机类</a></li><li><a class="dropdown__link" href="/personal-site/book-list">✒️ 文学类</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link heafer-life-icon" href="/personal-site/docs/category/just-paly"></a><a class="navbar__item navbar__link heafer-studio-icon" href="/personal-site/docs/category/summary"></a><a href="https://github.com/rcxxx/personal-site" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/personal-site/"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></a><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/OpenCV">OpenCV</a><button aria-label="Toggle the collapsible sidebar category &#x27;OpenCV&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/personal-site/docs/category/ml-dl">ML &amp; DL</a><button aria-label="Toggle the collapsible sidebar category &#x27;ML &amp; DL&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/about-AI">务必先看</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的基本概念">机器学习</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的基本概念">基本概念</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素">三个基本要素</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归">线性回归</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/PyTorch">PyTorch</a><button aria-label="Toggle the collapsible sidebar category &#x27;PyTorch&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/YOLO">YOLO</a><button aria-label="Toggle the collapsible sidebar category &#x27;YOLO&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/realsense">RealSense</a><button aria-label="Toggle the collapsible sidebar category &#x27;RealSense&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/Point-Cloud">Point Cloud</a><button aria-label="Toggle the collapsible sidebar category &#x27;Point Cloud&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_GujU"><div class="docItemContainer_Adtb"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/personal-site/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/personal-site/docs/category/ml-dl"><span itemprop="name">ML &amp; DL</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">机器学习</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">线性回归</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_aoJ5"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>机器学习的简单示例 —— 线性回归</h1></header><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>💡</h5></div><div class="admonition-content"><p><code>线性回归（Linear Regression）</code> 是机器学习和统计学中最基础广泛饮用的模型，是一种对自变量和因变量之间关系进行建模的回归分析</p><ul><li>自变量数量为 1 时称为 <code>简单回归</code> ，自变量数量大于 1 时称为 <code>多元回归</code></li></ul></div></div><p>从机器学习的角度来看，自变量就是样本的特征向量 $x \in \mathbb{R}^{D}$ （每一维对应一个自变量），因变量是标签 $y$ ，这里 $y \in \mathbb{R}$ 是连续值（实数或连续整数）</p><p>假设空间是一组参数化的线性函数</p><p>$$
f(x; w , b) = w^{T}x + b \tag{30}
$$</p><p>其中权重向量 $w \in \mathbb{R}^{D}$ 和偏置 $b \in \mathbb{R}$ 都是可学习的参数，函数 $f(x; w, b) \in \mathbb{R}$ 也称为线性模型</p><p>简单起见，将公式 $\color{blue}{(30)}$ 写为</p><p>$$
f(x; \hat w) = \hat w^{T} \hat x \tag{31}
$$</p><ul><li>$\hat w$ 为增广权重向量 ， $\hat x$ 为增广特征向量</li></ul><p>$$
\hat x = x 	\oplus 1 \triangleq \begin{bmatrix}
\qquad <!-- -->\<!-- --> x <!-- -->\<!-- --> <!-- -->\<!-- --> <!-- -->\<!-- --> 1
\end{bmatrix} = \begin{bmatrix}
x<em>{1} <!-- -->\<!-- --> \vdots <!-- -->\<!-- --> x</em>{D} <!-- -->\<!-- -->  <!-- -->\<!-- --> 1
\end{bmatrix} \tag{32}
$$</p><p>$$
\hat w = w \oplus b \triangleq \begin{bmatrix}
\qquad <!-- -->\<!-- --> w <!-- -->\<!-- --> <!-- -->\<!-- --> <!-- -->\<!-- --> b
\end{bmatrix} = \begin{bmatrix}
w<em>{1} <!-- -->\<!-- --> \vdots <!-- -->\<!-- --> w</em>{D} <!-- -->\<!-- -->  <!-- -->\<!-- --> b
\end{bmatrix} \tag{33}
$$</p><ul><li>$\oplus$ 定义为两个向量的拼接操作</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="参数学习">参数学习<a class="hash-link" href="#参数学习" title="Direct link to heading">​</a></h2><p>给定一组包含 $N$ 个训练样本的训练集 $D = \lbrace (x^{(n)}, y^{(n)})\rbrace_{n=1}^{N}$ ，我们希望能够学习一个最优的线性回归模型参数 $w$</p><p>这里介绍四种不同的参数估计方法</p><ul><li>经验风险最小化</li><li>结构风险最小化</li><li>最大似然估计</li><li>最大后验估计</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="经验风险最小化">经验风险最小化<a class="hash-link" href="#经验风险最小化" title="Direct link to heading">​</a></h3><p>线性回归的标签 $y$ 和模型输出都为连续的实数值，因此 <code>平方损失函数</code> 很适合衡量真实标签之间的差异</p><p>根据经验风险最小化准则，训练集 $\mathcal{D}$ 上的经验风险定义为</p><p>$$
\begin{aligned}
\mathcal{R}(w) &amp;= \sum<em>{n=1}^{N} \mathcal{L}(y^{(n)}, f(x^{(n)}; w))<!-- -->\<!-- -->
&amp;= \frac{1}{2} \sum</em>{n=1}^{N} (y^{(n)} - w^{T}x^{(n)})^{2}<!-- -->\<!-- -->
&amp;= \frac{1}{2} ||y-X^{T}w||^{2} \tag{34--36}
\end{aligned}
$$</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>💡</h5></div><div class="admonition-content"><p>这里的风险函数省略了 $\frac{1}{N}$ 来简化</p></div></div><ul><li>其中 $y=<!-- -->[y^{(1)}, \mathellipsis, y^{(N)}]<!-- -->^{T} \in \mathbb{R}^{N}$ 是由所有样本的真实标签组成的列向量，而 $X \in \mathbb{R}^{(D+1)\times N}$ 是由所有样本的输入特征 $x^{(1), \mathellipsis, x^{(N)}}$ 组成的矩阵</li></ul><p>$$
X = \begin{bmatrix}
x<em>{1}^{(1)} &amp; x</em>{1}^{(2)} &amp; \mathellipsis &amp; x<em>{1}^{(N)}<!-- -->\<!-- --> <!-- -->\<!-- -->
\vdots &amp; \vdots &amp; \ddots &amp; \vdots <!-- -->\<!-- --> <!-- -->\<!-- -->
x</em>{D}^{(1)} &amp; x<em>{D}^{(1)} &amp; \mathellipsis &amp; x</em>{D}^{(N)} <!-- -->\<!-- --> <!-- -->\<!-- -->
1 &amp; 1 &amp; \mathellipsis &amp; 1
\end{bmatrix} \tag{37}
$$</p><p>风险函数 $\mathcal{R}(w)$ 是关于 $w$ 的凸函数，其对 $w$ 的偏导数为</p><p>$$
\begin{aligned}
\frac{\partial \mathcal{R}(w)}{\partial w} &amp;= \frac{1}{2} \frac{\partial ||y-X^{T}w||^{2}}{\partial w} <!-- -->\<!-- -->
&amp;= -X(y-X^{T}w)
\end{aligned} \tag{38,~39}
$$</p><p>令 $\frac{\partial}{\partial w} \mathcal{R}(w) = 0$ ，得到的最优参数 $w^{*}$ 为</p><p>$$
\begin{aligned}
w^{*} &amp;= (XX^{T})^{-1}Xy<!-- -->\<!-- -->
&amp;= (\sum<em>{n=1}^{N} x^{(n)}(x^{(n)})^{T})^{-1}(\sum</em>{n=1}^{N} x^{(n)}y^{(n)}) \tag{40,~41}
\end{aligned}
$$</p><ul><li>这种求解线性回归参数的方法叫做 <code>最小二乘法（Least Square Method，LSM）</code></li></ul><p>书中给出的线性回归参数学习的示例</p><p><img loading="lazy" src="https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/ubuntu/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/least-square-method.png" class="img_ev3q"></p><p>在最小二乘法中， $XX^{T} \in \mathbb{R}^{(D+1)\times(D+1)}$ 必须存在逆矩阵，即 $XX^{T}$ 是满秩的 $(rank(XX^{T}) = D+1)$</p><ul><li>$X$ 中的行向量之间线性不相关，即每一个特征都和其他特征不相关</li></ul><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>✏️</h5></div><div class="admonition-content"><p>一种常见的 $XX^{T}$ 不可逆情况是样本数量 $N$ 小于特征数量 $(D+1)$ ，$XX^{T}$ 的秩为 $N$ ，这时会存在很多解 $w^{<em>}$ 使得 $\mathcal{R}(w^{</em>}) = 0$</p></div></div><p>当 $XX^{T}$ 不可逆时，可以通过这两种方法来估计参数</p><ol><li><p>先使用主成分分析等方法来预处理数据，消除不同特征之间的相关性，再使用最小二乘法</p></li><li><p>使用梯度下降来估计参数，先初始化 $w=0$ ，然后这个公式进行迭代</p></li></ol><p>$$
w \leftarrow w + \alpha X(y- X^{T}w) \tag{42}
$$</p><ul><li>其中 $\alpha$ 为学习率，这种利用梯度下降法来求解的方法也称为 <code>最小均方算法（Least Mean Squares，LMS）</code> </li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="结构风险最小化">结构风险最小化<a class="hash-link" href="#结构风险最小化" title="Direct link to heading">​</a></h3><p>最小二乘法中要保证 $XX^{T}$ 可逆，但是即使 $XX^{T}$ 可逆，如果特征之间有较大的 <code>多重共线性（Multicollinearity）</code> ，也会使 $XX^{T}$ 的逆在数值上无法准确计算</p><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>💡</h5></div><div class="admonition-content"><p><code>共线性（Collinearity）</code> 是指一个特征可以通过其他特征的线性组合来较准确的预测</p></div></div><p>数据集 $X$ 上的一些小的扰动就会导致 $(XX^{T})^{-1}$ 发生很大的改变，使得最小二乘法的计算变得很不稳定</p><p>为了解决这个问题，优秀的大佬们提出了 <code>岭回归（Ridge Regression）</code> ，即给 $XX^{T}$ 的对角线元素都加上一个常数 $\lambda$ 使得 $(XX^{T} + \lambda I)$ 满秩，最优的参数 $w^{*}$ 为</p><p>$$
w^{*} = (XX^{T} + \lambda I)^{-1}Xy \tag{43}
$$</p><ul><li>其中 $\lambda &gt; 0$ 为预先设置的超参数，$I$ 为单位矩阵</li></ul><p>岭回归的解 $w^{*}$ 可以看作 <code>结构风险最小化准则</code> 下的最小二乘法估计，其目标函数可以改写为</p><p>$$
\mathcal{R}(w) = \frac{1}{2}||y - X^{T} w||^{2} + \frac{1}{2} \lambda ||w||^{2} \tag{44}
$$</p><ul><li>其中 $\lambda &gt; 0$ 为正则化系数</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="最大似然估计">最大似然估计<a class="hash-link" href="#最大似然估计" title="Direct link to heading">​</a></h3><p>机器学习的任务可以分为两类</p><ol><li>样本的特征向量 $x$ 和标签 $y$ 之间存在位置函数关系 $y=h(x)$</li><li>条件概率 $p(y|x)$ 服从某个未知分布</li></ol><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><ul><li>最小二乘法属于第一类，直接建模 $x$ 和标签 $y$ 之间的函数关系</li><li>线性回归可以从建模条件概率 $p(y|x)$ 的角度进行参数估计</li></ul></div></div><p>假设标签 $y$ 为一个随机变量，并由函数 $f(x; w)=w^{T}x$ 加上一个随机噪声 $\epsilon$决定</p><p>$$
\begin{aligned}
y&amp;= f(x; w) + \epsilon<!-- -->\<!-- -->
&amp;= w^{T}x + \epsilon \tag{45,~46}
\end{aligned}
$$</p><ul><li>其中 $\epsilon$ 服从均值为 $0$、方差为 $\sigma^{2}$ 的高斯分布</li><li>$y$ 服从均值为 $w^{T}x$、方差为 $\sigma^{2}$ 的高斯分布</li></ul><p>$$
\begin{aligned}
p(y|x; w, \sigma) &amp;= \mathcal{N}(y; w^{T}x, \sigma^{2}) <!-- -->\<!-- -->
&amp;= \frac{1}{\sqrt{2\pi}\sigma} exp(- \frac{(y-w^{T}x)^2}{2\sigma^{2}})\tag{47,~48}
\end{aligned}
$$</p><ul><li>参数 $w$ 在训练集 $\mathcal{D}$ 上的 <strong><code>似然函数（Likelihood）</code></strong> 为</li></ul><p>$$
\begin{aligned}
p(y|X; w, \sigma) &amp;= \prod^{n}<em>{n=1}p(y^{(n)}|x^{(n)}; w, \sigma) <!-- -->\<!-- -->
&amp; = \prod^{n}</em>{n=1}p(y^{(n)}|w^{T}x^{(n)}, \sigma^{2})) \tag{49, ~50}
\end{aligned}
$$</p><ul><li>其中 $y = <!-- -->[y^{(1)}, \cdots, y^{(N)}]<!-- -->^{T}$ 为所有样本标签组成的向量， $X=<!-- -->[x^{(1)}, \cdots, x^{(N)}]<!-- -->$ 为所有样本特征向量组成的矩阵</li></ul><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p>为了方便计算，对似然函数取对数得到 <code>对数似然函数（Log Likelihood）</code></p><p>$$
\log p(y|X; w, \sigma) = \sum_{n=1}^{n} \log \mathcal{N}(y^{(n)}|w^{T}x^{(n)}, \sigma^{2})) \tag{51}
$$</p></div></div><p><strong><code>最大似然估计（Maximum Likelihood Estimation，MLE）</code></strong> 是指找到一组参数 $w$ 使得似然函数 $p(y|X; w, \sigma)$ 最大，等价于对数似然函数 $\log p(y|X; w, \sigma)$ 最大</p><ul><li>令 $\frac{\partial \log p(y|X; w, \sigma)}{\partial w} = 0$ 得到</li></ul><p>$$
w^{ML} = (XX^{T})^{-1}Xy \tag{52}
$$</p><p>最大似然估计的解和最小二乘法的解相同</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="参考">参考<a class="hash-link" href="#参考" title="Direct link to heading">​</a></h2><ul><li><strong>邱锡鹏，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/" target="_blank" rel="noopener noreferrer">https://nndl.github.io/</a>, 2020.</strong></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/rcxxx/docs/tree/master/docs/computer/cv/ml-dl/note/ml/线性回归.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_eYIM" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vbeJ"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">三个基本要素</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/personal-site/docs/category/PyTorch"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">PyTorch</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#参数学习" class="table-of-contents__link toc-highlight">参数学习</a><ul><li><a href="#经验风险最小化" class="table-of-contents__link toc-highlight">经验风险最小化</a></li><li><a href="#结构风险最小化" class="table-of-contents__link toc-highlight">结构风险最小化</a></li><li><a href="#最大似然估计" class="table-of-contents__link toc-highlight">最大似然估计</a></li></ul></li><li><a href="#参考" class="table-of-contents__link toc-highlight">参考</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://sinnammanyo.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">My Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/rcxxx" target="_blank" rel="noopener noreferrer" class="footer__link-item">My GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://wiki.wildwolf.pw/" target="_blank" rel="noopener noreferrer" class="footer__link-item">机器人队知识库<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 🌈RCXXX. Built with Docusaurus.</div></div></div></footer></div>
<script src="/personal-site/assets/js/runtime~main.f246873e.js"></script>
<script src="/personal-site/assets/js/main.6ada5662.js"></script>
</body>
</html>