---
id: 线性回归
title: 机器学习的简单示例 —— 线性回归
sidebar_label: 线性回归
---

:::info 💡
`线性回归（Linear Regression）` 是机器学习和统计学中最基础广泛饮用的模型，是一种对自变量和因变量之间关系进行建模的回归分析
- 自变量数量为 1 时称为 `简单回归` ，自变量数量大于 1 时称为 `多元回归`
:::

从机器学习的角度来看，自变量就是样本的特征向量 $x \in \mathbb{R}^{D}$ （每一维对应一个自变量），因变量是标签 $y$ ，这里 $y \in \mathbb{R}$ 是连续值（实数或连续整数）

假设空间是一组参数化的线性函数

$$
f(x; w , b) = w^{T}x + b \tag{30}
$$

其中权重向量 $w \in \mathbb{R}^{D}$ 和偏置 $b \in \mathbb{R}$ 都是可学习的参数，函数 $f(x; w, b) \in \mathbb{R}$ 也称为线性模型

简单起见，将公式 $\color{blue}{(30)}$ 写为

$$
f(x; \hat w) = \hat w^{T} \hat x \tag{31}
$$

- $\hat w$ 为增广权重向量 ， $\hat x$ 为增广特征向量

$$
\hat x = x 	\oplus 1 \triangleq \begin{bmatrix}
    \qquad \\ x \\ \\ \\ 1
\end{bmatrix} = \begin{bmatrix}
    x_{1} \\ \vdots \\ x_{D} \\  \\ 1
\end{bmatrix} \tag{32}
$$

$$
\hat w = w \oplus b \triangleq \begin{bmatrix}
    \qquad \\ w \\ \\ \\ b
\end{bmatrix} = \begin{bmatrix}
    w_{1} \\ \vdots \\ w_{D} \\  \\ b
\end{bmatrix} \tag{33}
$$

- $\oplus$ 定义为两个向量的拼接操作

## 参数学习
给定一组包含 $N$ 个训练样本的训练集 $D = \lbrace (x^{(n)}, y^{(n)})\rbrace_{n=1}^{N}$ ，我们希望能够学习一个最优的线性回归模型参数 $w$

这里介绍四种不同的参数估计方法
- 经验风险最小化
- 结构风险最小化
- 最大似然估计
- 最大后验估计

### 经验风险最小化
线性回归的标签 $y$ 和模型输出都为连续的实数值，因此 `平方损失函数` 很适合衡量真实标签之间的差异

根据经验风险最小化准则，训练集 $\mathcal{D}$ 上的经验风险定义为

$$
\begin{aligned}
\mathcal{R}(w) &= \sum_{n=1}^{N} \mathcal{L}(y^{(n)}, f(x^{(n)}; w))\\
&= \frac{1}{2} \sum_{n=1}^{N} (y^{(n)} - w^{T}x^{(n)})^{2}\\
&= \frac{1}{2} ||y-X^{T}w||^{2} \tag{34--36}
\end{aligned}
$$

:::tip 💡
这里的风险函数省略了 $\frac{1}{N}$ 来简化
:::

- 其中 $y=[y^{(1)}, \mathellipsis, y^{(N)}]^{T} \in \mathbb{R}^{N}$ 是由所有样本的真实标签组成的列向量，而 $X \in \mathbb{R}^{(D+1)\times N}$ 是由所有样本的输入特征 $x^{(1), \mathellipsis, x^{(N)}}$ 组成的矩阵

$$
X = \begin{bmatrix}
    x_{1}^{(1)} & x_{1}^{(2)} & \mathellipsis & x_{1}^{(N)}\\ \\
    \vdots & \vdots & \ddots & \vdots \\ \\
    x_{D}^{(1)} & x_{D}^{(1)} & \mathellipsis & x_{D}^{(N)} \\ \\
    1 & 1 & \mathellipsis & 1
\end{bmatrix} \tag{37}
$$

风险函数 $\mathcal{R}(w)$ 是关于 $w$ 的凸函数，其对 $w$ 的偏导数为

$$
\begin{aligned}
\frac{\partial \mathcal{R}(w)}{\partial w} &= \frac{1}{2} \frac{\partial ||y-X^{T}w||^{2}}{\partial w} \\
&= -X(y-X^{T}w)
\end{aligned} \tag{38,~39}
$$

令 $\frac{\partial}{\partial w} \mathcal{R}(w) = 0$ ，得到的最优参数 $w^{*}$ 为

$$
\begin{aligned}
w^{*} &= (XX^{T})^{-1}Xy\\
&= (\sum_{n=1}^{N} x^{(n)}(x^{(n)})^{T})^{-1}(\sum_{n=1}^{N} x^{(n)}y^{(n)}) \tag{40,~41}
\end{aligned}
$$

- 这种求解线性回归参数的方法叫做 `最小二乘法（Least Square Method，LSM）`

书中给出的线性回归参数学习的示例

![](https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/ubuntu/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/least-square-method.png)

在最小二乘法中， $XX^{T} \in \mathbb{R}^{(D+1)\times(D+1)}$ 必须存在逆矩阵，即 $XX^{T}$ 是满秩的 $(rank(XX^{T}) = D+1)$

- $X$ 中的行向量之间线性不相关，即每一个特征都和其他特征不相关

:::note ✏️
一种常见的 $XX^{T}$ 不可逆情况是样本数量 $N$ 小于特征数量 $(D+1)$ ，$XX^{T}$ 的秩为 $N$ ，这时会存在很多解 $w^{*}$ 使得 $\mathcal{R}(w^{*}) = 0$
:::

当 $XX^{T}$ 不可逆时，可以通过这两种方法来估计参数
1. 先使用主成分分析等方法来预处理数据，消除不同特征之间的相关性，再使用最小二乘法

2. 使用梯度下降来估计参数，先初始化 $w=0$ ，然后这个公式进行迭代

$$
w \leftarrow w + \alpha X(y- X^{T}w) \tag{42}
$$

- 其中 $\alpha$ 为学习率，这种利用梯度下降法来求解的方法也称为 `最小均方算法（Least Mean Squares，LMS）` 


### 结构风险最小化
最小二乘法中要保证 $XX^{T}$ 可逆，但是即使 $XX^{T}$ 可逆，如果特征之间有较大的 `多重共线性（Multicollinearity）` ，也会使 $XX^{T}$ 的逆在数值上无法准确计算

:::info 💡
`共线性（Collinearity）` 是指一个特征可以通过其他特征的线性组合来较准确的预测
:::

数据集 $X$ 上的一些小的扰动就会导致 $(XX^{T})^{-1}$ 发生很大的改变，使得最小二乘法的计算变得很不稳定

为了解决这个问题，优秀的大佬们提出了 `岭回归（Ridge Regression）` ，即给 $XX^{T}$ 的对角线元素都加上一个常数 $\lambda$ 使得 $(XX^{T} + \lambda I)$ 满秩，最优的参数 $w^{*}$ 为

$$
w^{*} = (XX^{T} + \lambda I)^{-1}Xy \tag{43}
$$

- 其中 $\lambda > 0$ 为预先设置的超参数，$I$ 为单位矩阵

岭回归的解 $w^{*}$ 可以看作 `结构风险最小化准则` 下的最小二乘法估计，其目标函数可以改写为

$$
\mathcal{R}(w) = \frac{1}{2}||y - X^{T} w||^{2} + \frac{1}{2} \lambda ||w||^{2}
$$

- 其中 $\lambda > 0$ 为正则化系数


### 最大似然估计


## 参考
- **邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.**

