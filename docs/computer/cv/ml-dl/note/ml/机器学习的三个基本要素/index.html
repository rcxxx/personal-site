<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-computer/cv/ml-dl/note/ml/机器学习的三个基本要素">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.21">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-rh="true">机器学习的三个基本要素 | Rcxxx&#x27;s Personal Site</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="机器学习的三个基本要素 | Rcxxx&#x27;s Personal Site"><meta data-rh="true" name="description" content="机器学习是从有限的观测数据中学习出具有一般性的规律，并且可以将总结出来的规律推广应用到未观测的样本上"><meta data-rh="true" property="og:description" content="机器学习是从有限的观测数据中学习出具有一般性的规律，并且可以将总结出来的规律推广应用到未观测的样本上"><link data-rh="true" rel="icon" href="/personal-site/./img/icons/lzumi-icon-05-128x128.jpg"><link data-rh="true" rel="canonical" href="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素"><link data-rh="true" rel="alternate" href="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素" hreflang="en"><link data-rh="true" rel="alternate" href="https://sinnammanyo.cn/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素" hreflang="x-default"><link rel="stylesheet" href="/personal-site/assets/css/styles.a67e7cdb.css">
<link rel="preload" href="/personal-site/assets/js/runtime~main.a6f5ee8e.js" as="script">
<link rel="preload" href="/personal-site/assets/js/main.839e1226.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/personal-site/"><div class="navbar__logo"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a aria-current="page" class="navbar__item navbar__link heafer-user-icon navbar__link--active" href="/personal-site/docs/"></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">📝Docs</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/personal-site/docs/category/devices">💻 PC</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/C-C_plus_plus">⌨️ programming</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/OpenCV">👀CV</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/RC-RM">🎖️robot</a></li><li><a class="dropdown__link" href="/personal-site/docs/category/Fusion 360">🔨3D Modeling</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">📚书单</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/personal-site/cs-book-list">⌨️ 计算机类</a></li><li><a class="dropdown__link" href="/personal-site/book-list">✒️ 文学类</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link heafer-life-icon" href="/personal-site/docs/category/just-paly"></a><a class="navbar__item navbar__link heafer-studio-icon" href="/personal-site/docs/category/summary"></a><a href="https://github.com/rcxxx/personal-site" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/personal-site/"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/personal-site/img/personal/ctrl-cv.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></a><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/OpenCV">OpenCV</a><button aria-label="Toggle the collapsible sidebar category &#x27;OpenCV&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/personal-site/docs/category/ml-dl">ML &amp; DL</a><button aria-label="Toggle the collapsible sidebar category &#x27;ML &amp; DL&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/about-AI">务必先看</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的基本概念">机器学习</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的基本概念">基本概念</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素">三个基本要素</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归">线性回归</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/PyTorch">PyTorch</a><button aria-label="Toggle the collapsible sidebar category &#x27;PyTorch&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/YOLO">YOLO</a><button aria-label="Toggle the collapsible sidebar category &#x27;YOLO&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/realsense">RealSense</a><button aria-label="Toggle the collapsible sidebar category &#x27;RealSense&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/personal-site/docs/category/Point-Cloud">Point Cloud</a><button aria-label="Toggle the collapsible sidebar category &#x27;Point Cloud&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_GujU"><div class="docItemContainer_Adtb"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/personal-site/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/personal-site/docs/category/ml-dl"><span itemprop="name">ML &amp; DL</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">机器学习</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">三个基本要素</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_aoJ5"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>机器学习的三个基本要素</h1></header><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>💡</h5></div><div class="admonition-content"><p>机器学习是从有限的观测数据中学习出具有一般性的规律，并且可以将总结出来的规律推广应用到未观测的样本上</p></div></div><p><strong>机器学习方法可以粗略的分为三个基本要素：模型、学习准则、优化算法</strong></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="一模型">一、模型<a class="hash-link" href="#一模型" title="Direct link to heading">​</a></h2><p>对于一个机器学习任务，首先要确定其输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$</p><p>不同的机器学习任务主要区别在于输出空间不同</p><p>例如：</p><ul><li>在二分类问题中 $y = \lbrace +1,-1 \rbrace$</li><li>在多分类问题中 $y = \lbrace 1,2, \mathellipsis,N \rbrace$ ，其中 $N$ 为问题种类</li><li>在回归问题中 $y = \mathbb{R}$ ，其中 $\mathbb{R}$ 为列向量</li></ul><p>对于样本空间中的 $(x,y) \in \mathcal{X} \times \mathcal{Y}$ ，假设 $x$ 和 $y$ 之间的关系可以通过一个未知的 <code>真实映射函数</code> $y = g(x)$ 或 <code>真实条件概率分布</code> $p_{r}(y|x)$ ，机器学习的目标就是找到这个函数或是这种概率分布</p><p>开始我们并不知道这个函数 $g(x)$ 或条件概率分布 $p_{r}(y|x)$ 的具体形式，只能根据经验来假设一个函数集合 $\mathcal{F}$ ，称为 <code>假设空间（Hypothesis Space）</code> ，然后通过观测其在训练集 $\mathcal{D}$ 上的特性，从中选择一个理想的 <code>假设（Hypothesis）</code> $f^* \in \mathcal{F}$</p><ul><li>假设空间通常为一个参数化的函数族</li></ul><p>$$
\mathcal{F} = \lbrace f(x;\theta)|\theta \in \mathbb{R}^D \rbrace \tag{5}
$$</p><p>其中 $f(x; \theta)$ 是参数为 $\theta$ 的函数，也称为 <code>模型（Model）</code> ， $D$ 为参数的数量</p><p>常见的假设空间分为线性和非线性两种，对应的模型 $f$ 分别称为线性模型和非线性模型</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="1线性模型">（1）线性模型<a class="hash-link" href="#1线性模型" title="Direct link to heading">​</a></h3><p><strong>线性模型的假设空间为一个参数化的线性函数族</strong></p><p>$$
f(x; \theta) = \omega^{T}x + b \tag{6}
$$</p><ul><li>其中参数 $\theta$ 包含了权重向量 $\omega$ 和偏置 $b$</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="2非线性模型">（2）非线性模型<a class="hash-link" href="#2非线性模型" title="Direct link to heading">​</a></h3><p><strong>广义的非线性模型可以写为多个非线性 <code>基函数</code> $\phi(x)$ 的线性组合</strong></p><p>$$
f(x; \theta) = \omega^{T} \phi(x) + b \tag{7}
$$</p><ul><li>$\phi(x) = <!-- -->[\phi_1(x), \phi_2(x), \mathellipsis, \phi_K(x)]<!-- -->^{T}$ 为 $K$ 个非线性基函数组成的向量，参数 $\theta$ 包含了权重向量 $\omega$ 和偏置 $b$</li></ul><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>✏️</h5></div><div class="admonition-content"><p>如果 $\phi(x)$ 本身为可学习的基函数，例如</p><p>$$
\phi<em>{k}(x) = h(\omega^{T}</em>{k} \phi^{&#x27;}(x) + b_k) , \forall 1 \leq k \leq K \tag{8}
$$</p><p>其中 $h(.)$ 为非线性函数， $\phi^{&#x27;}(x)$ 为另一组基函数，$\omega_k$ 和 $b_k$ 为可学习的参数， $f(x; \theta)$ 就等价于 <code>神经网络</code> 模型</p></div></div><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="二学习准则">二、学习准则<a class="hash-link" href="#二学习准则" title="Direct link to heading">​</a></h2><p>训练集 $\mathcal{D} = {\lbrace (x^{(n)}, y^{(n)})\rbrace}^{N}_{n=1}$ 是由 $N$ 个独立同分布的样本组成</p><p>每个样本 $(x,y) \in \mathcal{X} \times \mathcal{Y}$ 是从 $\mathcal{X}$ 和 $\mathcal{Y}$ 的联合空间中按照某种未知分布 $p_r(x, y)$ 独立地随机产生</p><ul><li>要求样本分布 $p_r(x, y)$ 必须是固定的，不会随时间变化，如果样本分布本身可变，就无法通过这些数据进行学习</li></ul><p>一个好的模型 $f(x, \theta^*)$ 应该在所有样本的可能取值上都与真实
映射函数 $y = g(x)$ 一致，即</p><p>$$
|f(x, \theta^*) - y| &lt; \epsilon, \qquad \forall(x, y) \in \mathcal{X} \times \mathcal{Y} \tag{9}
$$</p><p>或者与真实条件概率分布 $p_r(x|y)$ 一致，即</p><p>$$
|f_y(x, \theta^*) - p_r(x|y)| &lt; \epsilon, \qquad \forall(x, y) \in \mathcal{X} \times \mathcal{Y} \tag{10}
$$</p><ul><li>其中 $\epsilon$ 是一个很小的正数， $f_y(x, \theta^*)$ 为模型预测的条件概率分布中 $y$ 对应的概率</li></ul><p>模型 $f(x; \theta)$ 的好坏可通过 <code>期望风险（Expected Risk）</code> $\mathcal{R}(\theta)$ 来衡量，其定义为</p><p>$$
\mathcal{R}(\theta) = \mathbb{E}_{(x,y) \sim p_r(x,y)}<!-- -->[\mathcal{L}(y, f(x; \theta))]<!-- --> \tag{11}
$$</p><ul><li>其中 $p_r(x,y)$ 为真实的数据分布， $\mathcal{L}(y, f(x; \theta))$ 为损失函数，用来量化两个变量之间的差异</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="损失函数">损失函数<a class="hash-link" href="#损失函数" title="Direct link to heading">​</a></h3><p><strong>损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异</strong></p><p>几种常用的损失函数：</p><p><strong><code>0-1 损失函数</code></strong> 最直观的损失函数是模型在训练集上的错误率，即 <code>0-1 损失函数</code> </p><p>$$
\mathcal{L}(y, f(x; \theta)) = \left<!-- -->{<!-- -->
\begin{matrix}
0 ~, \qquad y=f(x;\theta)<!-- -->\<!-- -->
1 ~, \qquad y \neq f(x;\theta)
\end{matrix}
\right. \tag{12}
$$</p><ul><li>也可以看做是指示函数 $I(.)$</li></ul><p>$$
\mathcal{L}(y, f(x; \theta)) = I(y \neq f(x;\theta)) \tag{13}
$$</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>✏️</h5></div><div class="admonition-content"><p>虽然 <code>0-1 损失函数</code> 能够客观地评价模型的好坏，但是也有缺点</p><ul><li>不连续且导数为0，难以优化</li></ul><p>因此常用连续可微的损失函数替代</p></div></div><p><strong><code>平方损失函数</code></strong> <code>平方损失函数（Quadratic Loss Function）</code> 经常用在预测标签 $y$ 为实数值的任务中，定义为</p><p>$$
\mathcal{L}(y, f(x; \theta)) = \frac{1}{2}(y - f(x; \theta))^2 \tag{14}
$$</p><ul><li>平方损失函数一般不适用于分类问题</li></ul><p><strong><code>交叉熵损失函数</code></strong> <code>交叉熵损失函数（Cross-Entropy Loss Function）</code> 一般用于分类问题</p><p>假设样本的标签 $y \in \lbrace{1,\mathellipsis,N}\rbrace$ 为离散的类别，模型 $f(x; \theta) \in <!-- -->[0,1]<!-- -->^N$ 的输出位类别标签的条件概率分布，即</p><p>$$
p(y = c|x; \theta) = f_c(x; \theta) \tag{15}
$$</p><p>并满足</p><p>$$
f<em>c(x; \theta) \in <!-- -->[0, 1]<!-- -->, \qquad \sum</em>{c=1}^{N} f_c(x; \theta) = 1 \tag{16}
$$</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>🍉</h5></div><div class="admonition-content"><p>假设现在有 $N$ 个水果，可以用一个 $N$ 维的 <code>ont-hot</code> 向量 $y$ 来表示样本的标签，假设第 $k$ 个水果是西瓜，那么标签向量 $y$ 中只有第 $k$ 维的元素为 1 ，其余所有元素都为 0</p><p>标签向量 $y$ 可以看作样本标签的真实条件概率分布 $p_r(y|x)$ ，即第 $c$ 维 $(y_c, 1\leq c \leq N)$ 是类别为 $c$ 的真实条件概率</p><p>西瓜类别为 $k$ ，那么它属于第 $k$ 类的概率为 1 ，属于其他类的概率为 0</p></div></div><p>对于两个概率分布，一般可以用交叉熵来衡量差异</p><p>标签的真实分布 $y$ 和模型预测分布 $f(x; \theta)$ 之间的交叉熵为</p><p>$$
\begin{aligned}</p><p>\mathcal{L}(y, f(x; \theta)) &amp;= -y^{T} logf(x; \theta)\tag{17,~18}<!-- -->\<!-- -->
&amp;= - \sum_{c=1}^{N} y_clogf_c(x; \theta)
\end{aligned}
$$</p><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>eg.</h5></div><div class="admonition-content"><p>例如一个三分类问题，一个样本的标签向量为 $y = <!-- -->[0, 0, 1]<!-- -->^T$ ，模型预测的标签分部为 $f(x; \theta) = <!-- -->[0.3, 0.3 , 0.4]<!-- -->^T$ ，则它们的交叉熵为 </p><p>$$
-(0 \times log(0.3) + 0 \times log(0.3) + 1 \times log(0.4)) = - log(0.4)
$$</p><ul><li>因为 $y$ 为 <code>one-hot</code> 向量，公式 $\color{blue}{(18)}$ 也可以写为</li></ul><p>$$
\mathcal{L}(y, f(x; \theta)) = -logf_y(x; \theta) \tag{19}
$$</p><ul><li>其中 $f_y(x; \theta)$ 可以看作真实类别 $y$ 的似然函数</li></ul><p>因此，交叉熵损失函数也就是 <code>负对数似然函数（Negative Log-Likelihood）</code></p></div></div><p><strong><code>Hinge 损失函数</code></strong> 对于二分类问题，假设 $y$ 的取值为 $\lbrace +1,-1 \rbrace$ ， $f(x; \theta) \in \mathbb{R}$</p><p><code>Hinge 损失函数（Hinge Loss Function）</code> 为</p><p>$$
\begin{aligned}
\mathcal{L}(y, f(x; \theta)) &amp;= max(0, 1 - yf(x;\theta))<!-- -->\<!-- -->
&amp;\triangleq <!-- -->[1 - yf(x;\theta)]<!-- -->_{+} \tag{20, ~21}
\end{aligned}
$$</p><ul><li>其中 $<!-- -->[x]<!-- -->_{+} = max(0,x)$</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="风险最小化准则">风险最小化准则<a class="hash-link" href="#风险最小化准则" title="Direct link to heading">​</a></h3><p>一个好的模型 $f(x; \theta)$ 应当有一个比较小的期望错误</p><p>由于不知真实的数据分部和映射函数，无法计算期望风险 $\mathcal R(\theta)$ </p><p>给定一个训练集 $\mathcal{D} = {\lbrace (x^{(n)}, y^{(n)})\rbrace}_{n=1}^{N}$ ，我们可以计算的是 <code>经验风险（Empirical Risk）</code> ，即在训练集上的平均损失</p><p>$$
\mathcal{R}<em>{\mathcal{D}}^{emp} (\theta) = \frac{1}{N} \sum</em>{n=1}^{N} \mathcal{L}(y^{(n)},f(x^{(n)}; \theta)) \tag{22}
$$</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>✏️</h5></div><div class="admonition-content"><ul><li>一个可行的学习准则时找到一组参数 $\theta^*$ 使经验风险最小，即</li></ul><p>$$
\theta^* = \mathop{argmin}\limits<em>{\theta} \mathcal{R}</em>{\mathcal{D}}^{emp} \tag{23}
$$</p><p>这就是 <strong><code>经验风险最小化（Empirical Risk Minimization, ERM）</code></strong> 准则</p></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="过拟合与欠拟合">过拟合与欠拟合<a class="hash-link" href="#过拟合与欠拟合" title="Direct link to heading">​</a></h4><p><strong>过拟合</strong> </p><p>当训练集大小 $|\mathcal{D}|$ 趋向于无限大时，经验风险会越来越趋于期望风险，但是我们并不能获得无限的训练样本，而且训练样本通常是真实数据的一个很小的子集或者样本中包含一定的噪声数据，不能很好的反应全部数据的真实分布</p><p>经验风险最小化的原则很容易导致模型在训练集上错误率很低，但是在新的数据上错误率很高，这就是 <code>过拟合（Overfitting）</code></p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>📌</h5></div><div class="admonition-content"><p><strong>定义-过拟合</strong>：给定一个假设空间 $\mathcal{F}$ ，一个假设 $f \in \mathcal{F}$ ，如果存在其他的假设 $f^{&#x27;} \in \mathcal{F}$ ，使得在训练集上 $f$ 的损失比 $f^{&#x27;}$ 小，但是在整个样本空间上  $f^{&#x27;}$ 的损失比 $f$ 小，就说假设 $f$ 过度拟合训练数据</p></div></div><p>过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的</p><p>为了解决过拟合问题，一般在经验风险最小化的基础上在引入参数的 <code>正则化(Regularization)</code> 来限制模型能力，使其不要过度的最小化经验风险</p><ul><li>就是 <code>结构风险最小化（Structure Risk Minimization，SRM）</code> 准则</li></ul><p>$$
\begin{aligned}
\theta^* &amp;= \mathop{argmin}\limits<em>{\theta} \mathcal{R}</em>{\mathcal{D}}^{struct}(\theta)<!-- -->\<!-- -->
&amp;=\mathop{argmin}\limits<em>{\theta} \mathcal{R}</em>{\mathcal{D}}^{emp} + \frac{1}{2} \lambda ||\theta||^{2}<!-- -->\<!-- -->
&amp;=\mathop{argmin}\limits<em>{\theta} \frac{1}{N} \sum</em>{n=1}^{N} \mathcal{L}(y^{(n)}, f(x^{(n)}; \theta)) + \frac{1}{2} \lambda ||\theta||^{2} \tag{24,~25,~26}
\end{aligned}
$$</p><ul><li>其中 $||\theta||$ 是 $\ell_{2}$ 范数的正则化项，用来减少参数空间，避免过拟合； $\lambda$ 用来控制正则化的强度</li></ul><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>📌</h5></div><div class="admonition-content"><p>正则化项也可以使用其他函数，如 $\ell_{1}$ 范数</p><ul><li><a href="#%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96%E5%87%86%E5%88%99">参数正则化</a></li></ul></div></div><p><strong>欠拟合</strong></p><p><code>欠拟合（Underfitting）</code> 是与过拟合相反的概念，即模型不能很好的拟合训练数据，在训练集上的错误率比较高</p><p>欠拟合一般是由于模型能力不足造成的</p><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>✏️</h5></div><div class="admonition-content"><ul><li>关于过拟合和欠拟合</li></ul></div></div><p><strong>机器学习中的学习准则并不仅仅是拟合训练集上的数据，同时也要使得繁华错误最低</strong></p><ul><li>可以将机器学习看做一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题</li></ul><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="三优化算法">三、优化算法<a class="hash-link" href="#三优化算法" title="Direct link to heading">​</a></h2><p>训练集、假设空间和学习准则确定之后，如何找到最优的模型就成了一个 <code>最优化问题（Optimization）</code> 问题</p><ul><li>机器学习的训练过程其实就是最优化问题的求解过程</li></ul><p><strong><code>参数与超参数</code></strong></p><p>在机器学习中，优化可以分为参数优化和超参数优化</p><p>模型 $d(x; \theta)$ 中的 $\theta$ 称为模型的参数，可以通过优化算法进行学习</p><p>还有一类参数是用来定义模型结构或优化策略的，这类参数叫做 <code>超参数（Hyper-Parameter）</code></p><p>常见的超参数包括：</p><ul><li>聚类算法中的类别个数</li><li>梯度下降法中的步长</li><li>正则化项的系数</li><li>神经网络的层数</li><li>支持向量机中的核函数等</li></ul><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>✏️</h5></div><div class="admonition-content"><p>超参数的选取一般都是组合优化问题，很难通过优化算法来自动学习</p><ul><li><code>超参数优化</code> 是机器学习的一个经验性很强大技术，通常按照经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整</li></ul></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="梯度下降法">梯度下降法<a class="hash-link" href="#梯度下降法" title="Direct link to heading">​</a></h3><ul><li><strong>局部最优解</strong></li></ul><p><code>梯度下降法</code> 是机器学习中最简单最常用的优化算法，首先初始化参数 $\theta_{0}$ ，然后按以下迭代公式来计算训练集 $\mathcal{D}$ 上风险函数的最小值</p><p>$$
\begin{aligned}
\theta<em>{t+1} &amp;= \theta</em>{t} - \alpha \frac{\partial \mathcal{R}<em>{\mathcal{D}} \theta}{\partial \theta}<!-- -->\<!-- -->
&amp;= \theta</em>{t} - \alpha \frac{1}{N} \sum_{n=1}^{N} \frac{\partial \mathcal{L}(y^{(n)},f(x^{(n); \theta}))}{\partial \theta} \tag{27, ~28}
\end{aligned}
$$</p><ul><li>其中 $\theta_{t}$ 为第 $t$ 次迭代时代参数， $\alpha$ 为搜索步长，也被称为 <code>学习率（Learning Rate）</code></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="提前停止">提前停止<a class="hash-link" href="#提前停止" title="Direct link to heading">​</a></h3><p>前面提到了过拟合的概念，针对梯度下降的优化算法，除了加正则化项外，还可以通过提前停止来防止过拟合</p><p>除了训练集和测试集之外，有时还会使用一个 <code>验证集（Validation Set）</code> 来进行模型选择，测试模型在验证集上是否最优</p><p>在每次迭代时，把新得到的模型 $f(x; \theta)$ 在验证集上进行测试，并计算错误率，如果在验证集上的错误率不再下降，就停止迭代，这种策略就叫 <code>提前停止（Early Stop）</code></p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>✏️</h5></div><div class="admonition-content"><p>如果没有验证集，可以在训练集上划分出一个小比例的子集作为验证集</p></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="随机梯度下降法">随机梯度下降法<a class="hash-link" href="#随机梯度下降法" title="Direct link to heading">​</a></h3><p>在公式 $\color{blue}{(27)}$ 的梯度下降法中，目标函数是整个训练集上的风险函数，这种方式成为 <code>批量梯度下降法（Batch Gradient Descent，BGD）</code></p><p>这种方法每次迭代时需要计算每个样本上损失函数的梯度并求和，当训练集中的样本数量很大时，算法会有很高的空间复杂度，每次迭代计算的开销也很大</p><p>为了减少每次迭代的计算复杂度，我们也可以在每次迭代的时候只采集一个样本，计算这个样本损失函数的梯度并更新参数，即 <code>随机梯度下降法（Stochastic Gradient Descent，SGD）</code> ，也叫做 <code>增量梯度下降法</code></p><ul><li>经过足够次数的迭代时，随机梯度下降也可以收敛到局部最优解</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="小批量梯度下降法">小批量梯度下降法<a class="hash-link" href="#小批量梯度下降法" title="Direct link to heading">​</a></h3><p>随机梯度下降法的一个缺点是无法充分利用计算机的并行计算能力， <code>小批量梯度下降法（Mini-Batch Gradient Descent）</code> 是批量梯度下降和随机梯度下降的折中</p><p>每次迭代时随机选取一部分训练样本来计算梯度并更新参数，即能兼顾随机梯度下降法的优点，也可以提高训练效率</p><p>第 $t$ 次迭代时，随机选取含 $K$ 个样本的子集 $\mathcal{S}_{t}$ ，计算这个子集上每个样本的损失函数的梯度并取平均，然后更新参数</p><p>$$
\theta<em>{t+1} \leftarrow \theta</em>{t} - \alpha \frac{1}{K} \sum<em>{(x,y) \in \mathcal{S}</em>{t}} \frac{\partial \mathcal{L} (t, f(x; \theta))}{\partial \theta \tag{29}}
$$</p><ul><li>实际应用中，小批量随机梯度下降法有收敛快、计算开销小的优点，因此逐渐成为大规模机器学习中的主要优化算法</li></ul><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="参考">参考<a class="hash-link" href="#参考" title="Direct link to heading">​</a></h2><ul><li><strong>邱锡鹏，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/" target="_blank" rel="noopener noreferrer">https://nndl.github.io/</a>, 2020.</strong></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/rcxxx/docs/tree/master/docs/computer/cv/ml-dl/note/ml/机器学习的三个基本要素.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_eYIM" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vbeJ"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/personal-site/docs/computer/cv/ml-dl/note/ml/机器学习的基本概念"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">基本概念</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/personal-site/docs/computer/cv/ml-dl/note/ml/线性回归"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">线性回归</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#一模型" class="table-of-contents__link toc-highlight">一、模型</a><ul><li><a href="#1线性模型" class="table-of-contents__link toc-highlight">（1）线性模型</a></li><li><a href="#2非线性模型" class="table-of-contents__link toc-highlight">（2）非线性模型</a></li></ul></li><li><a href="#二学习准则" class="table-of-contents__link toc-highlight">二、学习准则</a><ul><li><a href="#损失函数" class="table-of-contents__link toc-highlight">损失函数</a></li><li><a href="#风险最小化准则" class="table-of-contents__link toc-highlight">风险最小化准则</a></li></ul></li><li><a href="#三优化算法" class="table-of-contents__link toc-highlight">三、优化算法</a><ul><li><a href="#梯度下降法" class="table-of-contents__link toc-highlight">梯度下降法</a></li><li><a href="#提前停止" class="table-of-contents__link toc-highlight">提前停止</a></li><li><a href="#随机梯度下降法" class="table-of-contents__link toc-highlight">随机梯度下降法</a></li><li><a href="#小批量梯度下降法" class="table-of-contents__link toc-highlight">小批量梯度下降法</a></li></ul></li><li><a href="#参考" class="table-of-contents__link toc-highlight">参考</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://sinnammanyo.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">My Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/rcxxx" target="_blank" rel="noopener noreferrer" class="footer__link-item">My GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://wiki.wildwolf.pw/" target="_blank" rel="noopener noreferrer" class="footer__link-item">机器人队知识库<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 🌈RCXXX. Built with Docusaurus.</div></div></div></footer></div>
<script src="/personal-site/assets/js/runtime~main.a6f5ee8e.js"></script>
<script src="/personal-site/assets/js/main.839e1226.js"></script>
</body>
</html>